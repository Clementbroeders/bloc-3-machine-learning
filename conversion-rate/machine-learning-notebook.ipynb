{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.preprocessing import  OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import make_scorer, f1_score, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('src/conversion_data_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>age</th>\n",
       "      <th>new_user</th>\n",
       "      <th>source</th>\n",
       "      <th>total_pages_visited</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>China</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>Direct</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UK</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>Ads</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Germany</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>Seo</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>Seo</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>Direct</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284575</th>\n",
       "      <td>US</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>Ads</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284576</th>\n",
       "      <td>US</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>Seo</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284577</th>\n",
       "      <td>US</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>Seo</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284578</th>\n",
       "      <td>US</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>Direct</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284579</th>\n",
       "      <td>US</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>Ads</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284580 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        country  age  new_user  source  total_pages_visited  converted\n",
       "0         China   22         1  Direct                    2          0\n",
       "1            UK   21         1     Ads                    3          0\n",
       "2       Germany   20         0     Seo                   14          1\n",
       "3            US   23         1     Seo                    3          0\n",
       "4            US   28         1  Direct                    3          0\n",
       "...         ...  ...       ...     ...                  ...        ...\n",
       "284575       US   36         1     Ads                    1          0\n",
       "284576       US   31         1     Seo                    2          0\n",
       "284577       US   41         1     Seo                    5          0\n",
       "284578       US   31         1  Direct                    4          0\n",
       "284579       US   26         0     Ads                    3          0\n",
       "\n",
       "[284580 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = data.drop('converted', axis = 1).columns\n",
    "numeric_indices = [1, 4]\n",
    "categorical_indices = [0, 2, 3]\n",
    "target_variable = 'converted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explanatory variables :  Index(['country', 'age', 'new_user', 'source', 'total_pages_visited'], dtype='object')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = data.loc[:, features_list]\n",
    "Y = data.loc[:, target_variable]\n",
    "\n",
    "print('Explanatory variables : ', X.columns)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dividing into train and test sets...\n",
      "...Done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Divide dataset Train set & Test set \n",
    "print(\"Dividing into train and test sets...\")\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state = 42, stratify = Y)\n",
    "print(\"...Done.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding categorical features and standardizing numerical features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done\n",
      "[[-1.27650481 -0.2618471   0.          0.          1.          1.\n",
      "   0.          0.        ]\n",
      " [-0.18867057 -0.56090876  0.          0.          0.          1.\n",
      "   0.          0.        ]\n",
      " [ 0.65742272 -0.56090876  0.          0.          1.          0.\n",
      "   0.          1.        ]\n",
      " [-0.9138934   0.93439955  0.          0.          1.          1.\n",
      "   0.          1.        ]\n",
      " [ 1.26177508 -0.56090876  0.          1.          0.          1.\n",
      "   0.          0.        ]]\n",
      "\n",
      "[[ 0.17264791 -0.25909297  0.          0.          1.          1.\n",
      "   0.          1.        ]\n",
      " [-0.07005486  0.04077964  0.          1.          0.          1.\n",
      "   0.          1.        ]\n",
      " [ 0.05129652  0.04077964  0.          0.          0.          1.\n",
      "   0.          1.        ]\n",
      " [ 2.47832424  0.04077964  0.          0.          1.          0.\n",
      "   0.          1.        ]\n",
      " [-1.28356872 -0.55896557  0.          0.          1.          1.\n",
      "   1.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Put here all the preprocessings\n",
    "print(\"Encoding categorical features and standardizing numerical features...\")\n",
    "numeric_transformer = StandardScaler()\n",
    "numeric_features = X.iloc[:,numeric_indices].columns\n",
    "\n",
    "categorical_transformer = OneHotEncoder(drop='first')\n",
    "categorical_features = X.iloc[:,categorical_indices].columns\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "\n",
    "X_test = preprocessor.fit_transform(X_test)\n",
    "print(\"...Done\")\n",
    "print(X_train[0:5,:])\n",
    "print()\n",
    "print(X_test[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done.\n",
      "Best hyperparameters :  {'colsample_bytree': 0.7718089712165844, 'gamma': 0.1917536675673416, 'learning_rate': 0.20634355033000254, 'max_delta_step': 1.6112364315351608, 'max_depth': 4, 'min_child_weight': 4, 'n_estimators': 178, 'reg_alpha': 0.1697110029051221, 'reg_lambda': 0.14190563929250422, 'scale_pos_weight': 1.7249311828776523, 'subsample': 0.8066239464686527}\n",
      "Best validation f1-score :  0.766656111913971\n",
      "\n",
      "f1-score on train set :  0.7733074361820199\n",
      "f1-score on test set :  0.7667766776677669\n",
      "\n",
      "Confusion matrix on train set : \n",
      "[[218822   1498]\n",
      " [  1770   5574]]\n",
      "\n",
      "Confusion matrix on test set : \n",
      "[[54674   406]\n",
      " [  442  1394]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train model XGBoost\n",
    "\n",
    "classifier = XGBClassifier()\n",
    "\n",
    "f1_scorer = make_scorer(f1_score)\n",
    "\n",
    "# params = {\n",
    "#     'max_depth': [8, 9, 10],\n",
    "#     'min_child_weight': [8, 9, 10],\n",
    "#     'n_estimators': [20, 30, 40],\n",
    "#     'reg_alpha': [0, 0.1, 0.2],\n",
    "#     'reg_lambda': [2, 2.5, 3]\n",
    "# }\n",
    "\n",
    "# classifier = GridSearchCV(classifier, \n",
    "#                           param_grid = params, \n",
    "#                           cv = 5,\n",
    "#                           scoring = f1_scorer,\n",
    "#                           n_jobs = -1)\n",
    "\n",
    "param_dist = {\n",
    "    'learning_rate': uniform(0.01, 0.3),\n",
    "    'n_estimators': randint(50, 500),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'min_child_weight': randint(1, 10),\n",
    "    'subsample': uniform(0.6, 0.4),\n",
    "    'colsample_bytree': uniform(0.6, 0.4),\n",
    "    'reg_alpha': uniform(0, 2),\n",
    "    'reg_lambda': uniform(0, 2),\n",
    "    'gamma': uniform(0, 0.2),\n",
    "    'scale_pos_weight': uniform(1, 5),\n",
    "    'max_delta_step': uniform(0, 5),\n",
    "}\n",
    "\n",
    "classifier = RandomizedSearchCV(estimator = classifier, \n",
    "                                param_distributions = param_dist, \n",
    "                                cv = 5,\n",
    "                                scoring = f1_scorer,\n",
    "                                n_iter = 50,\n",
    "                                n_jobs = -1)\n",
    "\n",
    "\n",
    "classifier.fit(X_train, Y_train)\n",
    "\n",
    "print(\"...Done.\")\n",
    "print(\"Best hyperparameters : \", classifier.best_params_)\n",
    "print(\"Best validation f1-score : \", classifier.best_score_)\n",
    "print()\n",
    "\n",
    "# Predictions on training set\n",
    "Y_train_pred = classifier.predict(X_train)\n",
    "\n",
    "# Predictions on test set\n",
    "Y_test_pred = classifier.predict(X_test)\n",
    "\n",
    "# F1-score\n",
    "print(\"f1-score on train set : \", f1_score(Y_train, Y_train_pred))\n",
    "print(\"f1-score on test set : \", f1_score(Y_test, Y_test_pred))\n",
    "print()\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"Confusion matrix on train set : \")\n",
    "print(confusion_matrix(Y_train, Y_train_pred))\n",
    "print()\n",
    "print(\"Confusion matrix on test set : \")\n",
    "print(confusion_matrix(Y_test, Y_test_pred))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done.\n",
      "Best hyperparameters :  {'tol': 8.497534359086438e-05, 'penalty': 'l2', 'max_iter': 1000, 'loss': 'modified_huber', 'learning_rate': 'adaptive', 'eta0': 0.002848035868435802, 'alpha': 0.0005336699231206312}\n",
      "Best validation f1-score :  0.7570075249045034\n",
      "\n",
      "f1-score on train set :  0.7582806963487907\n",
      "f1-score on test set :  0.7536054004295796\n",
      "\n",
      "Confusion matrix on train set : \n",
      "[[219604    730]\n",
      " [  2408   4922]]\n",
      "\n",
      "Confusion matrix on test set : \n",
      "[[54885   181]\n",
      " [  622  1228]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train model SGDClassifier\n",
    "\n",
    "classifier = SGDClassifier()\n",
    "\n",
    "f1_scorer = make_scorer(f1_score)\n",
    "\n",
    "# params = {\n",
    "# }\n",
    "\n",
    "# classifier = GridSearchCV(classifier, \n",
    "#                           param_grid = params, \n",
    "#                           cv = 5,\n",
    "#                           scoring = f1_scorer,\n",
    "#                           n_jobs = -1)\n",
    "\n",
    "param_dist = {\n",
    "    'loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
    "    'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "    'alpha': np.logspace(-4, 4, 100),\n",
    "    'learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "    'eta0': np.logspace(-4, 0, 100),\n",
    "    'max_iter': [1000, 2000, 3000],\n",
    "    'tol': np.logspace(-5, -1, 100),\n",
    "}\n",
    "\n",
    "classifier = RandomizedSearchCV(estimator = classifier, \n",
    "                                param_distributions = param_dist, \n",
    "                                cv = 5,\n",
    "                                scoring = f1_scorer,\n",
    "                                n_iter = 100,\n",
    "                                random_state = 0,\n",
    "                                n_jobs = -1)\n",
    "\n",
    "\n",
    "classifier.fit(X_train, Y_train)\n",
    "\n",
    "print(\"...Done.\")\n",
    "print(\"Best hyperparameters : \", classifier.best_params_)\n",
    "print(\"Best validation f1-score : \", classifier.best_score_)\n",
    "print()\n",
    "\n",
    "# Predictions on training set\n",
    "Y_train_pred = classifier.predict(X_train)\n",
    "\n",
    "# Predictions on test set\n",
    "Y_test_pred = classifier.predict(X_test)\n",
    "\n",
    "# F1-score\n",
    "print(\"f1-score on train set : \", f1_score(Y_train, Y_train_pred))\n",
    "print(\"f1-score on test set : \", f1_score(Y_test, Y_test_pred))\n",
    "print()\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"Confusion matrix on train set : \")\n",
    "print(confusion_matrix(Y_train, Y_train_pred))\n",
    "print()\n",
    "print(\"Confusion matrix on test set : \")\n",
    "print(confusion_matrix(Y_test, Y_test_pred))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done.\n",
      "Best hyperparameters :  {'base_estimator__C': 6, 'n_estimators': 11}\n",
      "Best validation f1-score :  0.7646726428387098\n",
      "\n",
      "f1-score on train set :  0.7650479571029379\n",
      "f1-score on test set :  0.7588075880758809\n",
      "\n",
      "Confusion matrix on train set : \n",
      "[[219488    846]\n",
      " [  2265   5065]]\n",
      "\n",
      "Confusion matrix on test set : \n",
      "[[54855   211]\n",
      " [  590  1260]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train model bagging logistic regression\n",
    "\n",
    "logistic_regression = LogisticRegression(max_iter = 1000)\n",
    "classifier = BaggingClassifier(logistic_regression)\n",
    "\n",
    "f1_scorer = make_scorer(f1_score)\n",
    "\n",
    "# params = {\n",
    "#     'base_estimator__C': [10, 15], \n",
    "#     'n_estimators': [30, 40] \n",
    "# }\n",
    "\n",
    "# classifier = GridSearchCV(classifier, \n",
    "#                           param_grid = params, \n",
    "#                           cv = 5,\n",
    "#                           scoring = f1_scorer,\n",
    "#                           n_jobs = -1)\n",
    "\n",
    "param_dist = {\n",
    "    'base_estimator__C': randint(1, 20),\n",
    "    'n_estimators': randint(10, 100),\n",
    "}\n",
    "\n",
    "classifier = RandomizedSearchCV(estimator = classifier, \n",
    "                                param_distributions = param_dist, \n",
    "                                cv = 5,\n",
    "                                scoring = f1_scorer,\n",
    "                                n_iter = 10,\n",
    "                                random_state = 42,\n",
    "                                n_jobs = -1)\n",
    "\n",
    "classifier.fit(X_train, Y_train)\n",
    "\n",
    "print(\"...Done.\")\n",
    "print(\"Best hyperparameters : \", classifier.best_params_)\n",
    "print(\"Best validation f1-score : \", classifier.best_score_)\n",
    "print()\n",
    "\n",
    "# Predictions on training set\n",
    "Y_train_pred = classifier.predict(X_train)\n",
    "\n",
    "# Predictions on test set\n",
    "Y_test_pred = classifier.predict(X_test)\n",
    "\n",
    "# F1-score\n",
    "print(\"f1-score on train set : \", f1_score(Y_train, Y_train_pred))\n",
    "print(\"f1-score on test set : \", f1_score(Y_test, Y_test_pred))\n",
    "print()\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"Confusion matrix on train set : \")\n",
    "print(confusion_matrix(Y_train, Y_train_pred))\n",
    "print()\n",
    "print(\"Confusion matrix on test set : \")\n",
    "print(confusion_matrix(Y_test, Y_test_pred))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done.\n",
      "Best hyperparameters :  {'base_estimator__C': 7, 'n_estimators': 189}\n",
      "Best validation f1-score :  0.7640180246934657\n",
      "\n",
      "f1-score on train set :  0.7638668779714739\n",
      "f1-score on test set :  0.7600721587492484\n",
      "\n",
      "Confusion matrix on train set : \n",
      "[[219474    860]\n",
      " [  2269   5061]]\n",
      "\n",
      "Confusion matrix on test set : \n",
      "[[54854   212]\n",
      " [  586  1264]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train model bagging logistic regression\n",
    "\n",
    "logistic_regression = LogisticRegression(max_iter = 1000)\n",
    "classifier = AdaBoostClassifier(logistic_regression)\n",
    "\n",
    "f1_scorer = make_scorer(f1_score)\n",
    "\n",
    "# params = {\n",
    "#     'base_estimator__C': [1, 5, 10], \n",
    "#     'n_estimators': [20, 30, 40] \n",
    "# }\n",
    "\n",
    "# classifier = GridSearchCV(classifier, \n",
    "#                           param_grid = params, \n",
    "#                           cv = 5,\n",
    "#                           scoring = f1_scorer,\n",
    "#                           n_jobs = -1)\n",
    "\n",
    "param_dist = {\n",
    "    'base_estimator__C': randint(1, 20),\n",
    "    'n_estimators': randint(10, 200),\n",
    "}\n",
    "\n",
    "classifier = RandomizedSearchCV(estimator = classifier, \n",
    "                                param_distributions = param_dist, \n",
    "                                cv = 5,\n",
    "                                scoring = f1_scorer,\n",
    "                                n_iter = 10,\n",
    "                                random_state = 42,\n",
    "                                n_jobs = -1)\n",
    "\n",
    "classifier.fit(X_train, Y_train)\n",
    "\n",
    "print(\"...Done.\")\n",
    "print(\"Best hyperparameters : \", classifier.best_params_)\n",
    "print(\"Best validation f1-score : \", classifier.best_score_)\n",
    "print()\n",
    "\n",
    "# Predictions on training set\n",
    "Y_train_pred = classifier.predict(X_train)\n",
    "\n",
    "# Predictions on test set\n",
    "Y_test_pred = classifier.predict(X_test)\n",
    "\n",
    "# F1-score\n",
    "print(\"f1-score on train set : \", f1_score(Y_train, Y_train_pred))\n",
    "print(\"f1-score on test set : \", f1_score(Y_test, Y_test_pred))\n",
    "print()\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"Confusion matrix on train set : \")\n",
    "print(confusion_matrix(Y_train, Y_train_pred))\n",
    "print()\n",
    "print(\"Confusion matrix on test set : \")\n",
    "print(confusion_matrix(Y_test, Y_test_pred))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done.\n",
      "Best hyperparameters :  {'learning_rate': 0.27753190023462393, 'max_depth': 3, 'min_samples_leaf': 13, 'min_samples_split': 3, 'n_estimators': 49}\n",
      "Best validation f1-score :  0.7634423993794718\n",
      "\n",
      "f1-score on train set :  0.7651727004761545\n",
      "f1-score on test set :  0.7566425120772946\n",
      "\n",
      "Confusion matrix on train set : \n",
      "[[219495    839]\n",
      " [  2268   5062]]\n",
      "\n",
      "Confusion matrix on test set : \n",
      "[[54857   209]\n",
      " [  597  1253]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train model RandomForest\n",
    "\n",
    "classifier = GradientBoostingClassifier()\n",
    "\n",
    "f1_scorer = make_scorer(f1_score)\n",
    "\n",
    "# params = {\n",
    "#     'max_depth': [10, 11, 12],\n",
    "#     'min_samples_leaf': [9, 10, 11],\n",
    "#     'min_samples_split': [10, 11, 12],\n",
    "#     'n_estimators': [20, 30, 40]\n",
    "# }\n",
    "\n",
    "# classifier = GridSearchCV(classifier, \n",
    "#                           param_grid = params, \n",
    "#                           cv = 5,\n",
    "#                           scoring = f1_scorer,\n",
    "#                           n_jobs = -1)\n",
    "\n",
    "param_dist = {\n",
    "    'learning_rate': uniform(0.01, 0.3),\n",
    "    'n_estimators': randint(10, 100),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'min_samples_leaf': randint(1, 20),\n",
    "    'min_samples_split': randint(2, 20),\n",
    "}\n",
    "\n",
    "classifier = RandomizedSearchCV(estimator = classifier, \n",
    "                                param_distributions = param_dist, \n",
    "                                cv = 5,\n",
    "                                scoring = f1_scorer,\n",
    "                                n_iter = 10,\n",
    "                                random_state = 0,\n",
    "                                n_jobs = -1)\n",
    "\n",
    "classifier.fit(X_train, Y_train)\n",
    "\n",
    "print(\"...Done.\")\n",
    "print(\"Best hyperparameters : \", classifier.best_params_)\n",
    "print(\"Best validation f1-score : \", classifier.best_score_)\n",
    "print()\n",
    "\n",
    "# Predictions on training set\n",
    "Y_train_pred = classifier.predict(X_train)\n",
    "\n",
    "# Predictions on test set\n",
    "Y_test_pred = classifier.predict(X_test)\n",
    "\n",
    "# F1-score\n",
    "print(\"f1-score on train set : \", f1_score(Y_train, Y_train_pred))\n",
    "print(\"f1-score on test set : \", f1_score(Y_test, Y_test_pred))\n",
    "print()\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"Confusion matrix on train set : \")\n",
    "print(confusion_matrix(Y_train, Y_train_pred))\n",
    "print()\n",
    "print(\"Confusion matrix on test set : \")\n",
    "print(confusion_matrix(Y_test, Y_test_pred))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done.\n",
      "Best hyperparameters :  {'bootstrap': True, 'criterion': 'entropy', 'max_depth': 8, 'max_features': 'log2', 'min_samples_leaf': 9, 'min_samples_split': 19, 'n_estimators': 89}\n",
      "Best validation f1-score :  0.7581701326685064\n",
      "\n",
      "f1-score on train set :  0.76501325255585\n",
      "f1-score on test set :  0.7543859649122807\n",
      "\n",
      "Confusion matrix on train set : \n",
      "[[219510    824]\n",
      " [  2279   5051]]\n",
      "\n",
      "Confusion matrix on test set : \n",
      "[[54857   209]\n",
      " [  603  1247]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train model RandomForest\n",
    "\n",
    "classifier = RandomForestClassifier()\n",
    "\n",
    "f1_scorer = make_scorer(f1_score)\n",
    "\n",
    "# params = {\n",
    "#     'max_depth': [10, 11, 12],\n",
    "#     'min_samples_leaf': [9, 10, 11],\n",
    "#     'min_samples_split': [10, 11, 12],\n",
    "#     'n_estimators': [20, 30, 40]\n",
    "# }\n",
    "\n",
    "# classifier = GridSearchCV(classifier, \n",
    "#                           param_grid = params, \n",
    "#                           cv = 5,\n",
    "#                           scoring = f1_scorer,\n",
    "#                           n_jobs = -1)\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': randint(10, 200),\n",
    "    'max_depth': randint(3, 20),\n",
    "    'min_samples_leaf': randint(1, 20),\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'max_features': ['auto', 'sqrt', 'log2', None],\n",
    "    'bootstrap': [True, False], \n",
    "    'criterion': ['gini', 'entropy'],\n",
    "}\n",
    "\n",
    "classifier = RandomizedSearchCV(estimator = classifier, \n",
    "                                param_distributions = param_dist, \n",
    "                                cv = 5,\n",
    "                                scoring = f1_scorer,\n",
    "                                n_iter = 10,\n",
    "                                random_state = 0,\n",
    "                                n_jobs = -1)\n",
    "\n",
    "classifier.fit(X_train, Y_train)\n",
    "\n",
    "print(\"...Done.\")\n",
    "print(\"Best hyperparameters : \", classifier.best_params_)\n",
    "print(\"Best validation f1-score : \", classifier.best_score_)\n",
    "print()\n",
    "\n",
    "# Predictions on training set\n",
    "Y_train_pred = classifier.predict(X_train)\n",
    "\n",
    "# Predictions on test set\n",
    "Y_test_pred = classifier.predict(X_test)\n",
    "\n",
    "# F1-score\n",
    "print(\"f1-score on train set : \", f1_score(Y_train, Y_train_pred))\n",
    "print(\"f1-score on test set : \", f1_score(Y_test, Y_test_pred))\n",
    "print()\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"Confusion matrix on train set : \")\n",
    "print(confusion_matrix(Y_train, Y_train_pred))\n",
    "print()\n",
    "print(\"Confusion matrix on test set : \")\n",
    "print(confusion_matrix(Y_test, Y_test_pred))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done.\n",
      "Best hyperparameters :  {'C': 5.588135039273247, 'coef0': -0.2848106336275805, 'degree': 4, 'gamma': 'auto', 'kernel': 'linear', 'shrinking': False}\n",
      "Best validation f1-score :  0.7602562032982104\n",
      "\n",
      "f1-score on train set :  0.7598686722150111\n",
      "f1-score on test set :  0.7550151975683891\n",
      "\n",
      "Confusion matrix on train set : \n",
      "[[219543    791]\n",
      " [  2354   4976]]\n",
      "\n",
      "Confusion matrix on test set : \n",
      "[[54868   198]\n",
      " [  608  1242]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train model SVC\n",
    "\n",
    "classifier = SVC()\n",
    "\n",
    "f1_scorer = make_scorer(f1_score)\n",
    "\n",
    "# params = {\n",
    "#     'C': [0.1, 1.0, 10],\n",
    "#     'gamma': [0.1, 1.0, 10],\n",
    "#     'kernel': ['linear']\n",
    "# }\n",
    "\n",
    "# classifier = GridSearchCV(classifier, \n",
    "#                           param_grid = params, \n",
    "#                           cv = 5,\n",
    "#                           scoring = f1_scorer,\n",
    "#                           n_jobs = -1)\n",
    "\n",
    "param_dist = {\n",
    "    'C': uniform(0.1, 10), \n",
    "    'kernel': ['linear'],\n",
    "    'degree': randint(1, 10),\n",
    "    'gamma': ['scale', 'auto', uniform(0.01, 1.0)],\n",
    "    'coef0': uniform(-1, 1),\n",
    "    'shrinking': [True, False],\n",
    "}\n",
    "\n",
    "classifier = RandomizedSearchCV(estimator = classifier, \n",
    "                                param_distributions = param_dist, \n",
    "                                cv = 5,\n",
    "                                scoring = f1_scorer,\n",
    "                                n_iter = 1,\n",
    "                                random_state = 0,\n",
    "                                n_jobs = -1)\n",
    "\n",
    "classifier.fit(X_train, Y_train)\n",
    "\n",
    "print(\"...Done.\")\n",
    "print(\"Best hyperparameters : \", classifier.best_params_)\n",
    "print(\"Best validation f1-score : \", classifier.best_score_)\n",
    "print()\n",
    "\n",
    "# Predictions on training set\n",
    "Y_train_pred = classifier.predict(X_train)\n",
    "\n",
    "# Predictions on test set\n",
    "Y_test_pred = classifier.predict(X_test)\n",
    "\n",
    "# F1-score\n",
    "print(\"f1-score on train set : \", f1_score(Y_train, Y_train_pred))\n",
    "print(\"f1-score on test set : \", f1_score(Y_test, Y_test_pred))\n",
    "print()\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"Confusion matrix on train set : \")\n",
    "print(confusion_matrix(Y_train, Y_train_pred))\n",
    "print()\n",
    "print(\"Confusion matrix on test set : \")\n",
    "print(confusion_matrix(Y_test, Y_test_pred))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN ON ALL DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.7718089712165844, device=None,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, feature_types=None, gamma=0.1917536675673416,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.20634355033000254,\n",
       "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=1.6112364315351608, max_depth=4, max_leaves=None,\n",
       "              min_child_weight=4, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=178, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.7718089712165844, device=None,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, feature_types=None, gamma=0.1917536675673416,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.20634355033000254,\n",
       "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=1.6112364315351608, max_depth=4, max_leaves=None,\n",
       "              min_child_weight=4, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=178, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.7718089712165844, device=None,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, feature_types=None, gamma=0.1917536675673416,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.20634355033000254,\n",
       "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=1.6112364315351608, max_depth=4, max_leaves=None,\n",
       "              min_child_weight=4, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=178, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Concatenate our train and test set to train your best classifier on all data with labels\n",
    "# X = np.append(X_train,X_test,axis=0)\n",
    "# Y = np.append(Y_train,Y_test)\n",
    "\n",
    "# classifier.best_estimator_.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction set (without labels) : (31620, 5)\n"
     ]
    }
   ],
   "source": [
    "# # Read data without labels\n",
    "# data_without_labels = pd.read_csv('src/conversion_data_test.csv')\n",
    "# print('Prediction set (without labels) :', data_without_labels.shape)\n",
    "\n",
    "# # Warning : check consistency of features_list (must be the same than the features \n",
    "# # used by your best classifier)\n",
    "# # features_list = ['total_pages_visited']\n",
    "# X_without_labels = data_without_labels.loc[:, features_list]\n",
    "\n",
    "# # Convert pandas DataFrames to numpy arrays before using scikit-learn\n",
    "# # print(\"Convert pandas DataFrames to numpy arrays...\")\n",
    "# # X_without_labels = X_without_labels.values\n",
    "# # print(\"...Done\")\n",
    "\n",
    "# # print(X_without_labels[0:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding categorical features and standardizing numerical features...\n",
      "...Done\n",
      "[[-0.31275763  3.3393783   0.          1.          0.          0.\n",
      "   0.          1.        ]\n",
      " [-1.04086595  0.04077964  0.          1.          0.          1.\n",
      "   1.          0.        ]\n",
      " [ 0.17264791 -1.15871078  0.          0.          0.          1.\n",
      "   0.          1.        ]\n",
      " [ 0.17264791  0.34065224  0.          0.          1.          1.\n",
      "   0.          0.        ]\n",
      " [-0.67681179 -0.55896557  0.          0.          0.          0.\n",
      "   0.          1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# # WARNING : PUT HERE THE SAME PREPROCESSING AS FOR YOUR TEST SET\n",
    "# # CHECK YOU ARE USING X_without_labels\n",
    "# print(\"Encoding categorical features and standardizing numerical features...\")\n",
    "\n",
    "# X_without_labels = preprocessor.transform(X_without_labels)\n",
    "# print(\"...Done\")\n",
    "# print(X_without_labels[0:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make predictions and dump to file\n",
    "# # WARNING : MAKE SURE THE FILE IS A CSV WITH ONE COLUMN NAMED 'converted' AND NO INDEX !\n",
    "# # WARNING : FILE NAME MUST HAVE FORMAT 'conversion_data_test_predictions_[name].csv'\n",
    "# # where [name] is the name of your team/model separated by a '-'\n",
    "# # For example : [name] = AURELIE-model1\n",
    "# data = {\n",
    "#     'converted': classifier.best_estimator_.predict(X_without_labels)\n",
    "# }\n",
    "\n",
    "# Y_predictions = pd.DataFrame(columns=['converted'],data=data)\n",
    "# Y_predictions.to_csv('src/conversion_data_test_predictions_EXAMPLE.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
